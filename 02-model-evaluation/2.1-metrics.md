# 2.1 评测指标体系

## 概述

评测指标（Evaluation Metrics）是量化模型表现的数学工具。不同的任务类型需要不同的评测指标，选择合适的指标对于准确评估模型能力至关重要。

本节将系统介绍 LLM 评测中常用的指标体系，包括其数学定义、计算方法、适用场景和局限性。

## 指标分类框架

根据评测任务的性质，可将指标分为以下几类：

| 类别 | 适用任务 | 代表指标 |
|------|----------|----------|
| 精确匹配类 | 问答、分类 | Exact Match, Accuracy |
| 文本重叠类 | 翻译、摘要 | BLEU, ROUGE |
| 语义相似类 | 开放生成 | BERTScore, BLEURT |
| 代码执行类 | 代码生成 | Pass@k |
| 语言模型类 | 模型基础能力 | Perplexity |

---

## 一、精确匹配类指标

### 1.1 Exact Match (EM)

#### 定义

Exact Match 是最简单直接的评测方式，判断模型输出是否与标准答案完全一致。

$$
EM = \frac{\text{完全匹配的样本数}}{\text{总样本数}}
$$

#### 计算示例

| 标准答案 | 模型输出 | 是否匹配 |
|----------|----------|----------|
| Paris | Paris | ✓ |
| Paris | paris | ✗ |
| Paris | Paris, France | ✗ |

#### 适用场景

- 有唯一标准答案的问答任务
- 分类任务
- 信息抽取任务

#### 局限性

- **对格式敏感**：大小写、标点、空格都会影响结果
- **无法处理语义等价**："北京"和"Beijing"会被判为不匹配
- **不适合开放生成**：多种表述都可能正确的任务

#### 改进变体

- **Normalized EM**：统一大小写、去除标点后比较
- **F1 Score**：基于 token 重叠计算，更宽松

### 1.2 Accuracy（准确率）

#### 定义

准确率是分类任务最常用的指标，计算正确预测的比例。

$$
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$

其中：
- TP (True Positive)：正确预测为正类
- TN (True Negative)：正确预测为负类
- FP (False Positive)：错误预测为正类
- FN (False Negative)：错误预测为负类

#### 在 LLM 评测中的应用

多选题评测（如 MMLU）通常使用准确率：

```
问题：法国的首都是？
A. 伦敦  B. 巴黎  C. 柏林  D. 马德里

模型选择：B
正确答案：B
结果：正确 (+1)
```

#### 局限性

- **类别不平衡问题**：当正负样本比例悬殊时，准确率可能失真
- **不反映置信度**：无法区分"确定正确"和"猜对"

---

## 二、文本重叠类指标

### 2.1 BLEU (Bilingual Evaluation Understudy)

#### 背景

BLEU 由 IBM 于 2002 年提出，最初用于机器翻译评估，是 NLP 领域最经典的自动评测指标之一。

#### 核心思想

通过计算模型输出与参考文本的 **n-gram 重叠程度** 来评估质量。

#### 数学定义

$$
BLEU = BP \cdot \exp\left(\sum_{n=1}^{N} w_n \log p_n\right)
$$

其中：
- $p_n$ 是 n-gram 精确率
- $w_n$ 是权重（通常取均匀权重 1/N）
- $BP$ 是简短惩罚因子（Brevity Penalty）

**n-gram 精确率**：

$$
p_n = \frac{\sum_{ngram \in candidate} Count_{clip}(ngram)}{\sum_{ngram \in candidate} Count(ngram)}
$$

**简短惩罚**（防止生成过短文本获得高分）：

$$
BP = \begin{cases}
1 & \text{if } c > r \\
e^{(1-r/c)} & \text{if } c \leq r
\end{cases}
$$

其中 $c$ 是候选文本长度，$r$ 是参考文本长度。

#### 计算示例

**参考译文**：The cat sat on the mat.

**候选译文**：The cat is on the mat.

| n-gram | 参考文本中的 n-gram | 候选文本中的 n-gram | 匹配数 |
|--------|----------------------|----------------------|--------|
| 1-gram | the, cat, sat, on, the, mat | the, cat, is, on, the, mat | 5/6 |
| 2-gram | the cat, cat sat, sat on, on the, the mat | the cat, cat is, is on, on the, the mat | 3/5 |

#### BLEU 变体

- **BLEU-1**：只看 1-gram（单词匹配）
- **BLEU-4**：综合 1-4 gram（最常用）
- **SacreBLEU**：标准化实现，确保可复现

#### 适用场景

- 机器翻译质量评估
- 文本摘要（辅助指标）
- 对话生成（参考）

#### 局限性

| 问题 | 说明 |
|------|------|
| 忽略语义 | "I love you" 和 "I adore you" 得分很低 |
| 忽略流畅性 | 词序混乱但词汇匹配的文本可能高分 |
| 参考依赖 | 只有一个参考时不够鲁棒 |
| 语言依赖 | 对中文等无空格分词的语言需要预处理 |

### 2.2 ROUGE (Recall-Oriented Understudy for Gisting Evaluation)

#### 背景

ROUGE 由 USC 的 Chin-Yew Lin 于 2004 年提出，专门用于文本摘要评估。

#### 与 BLEU 的核心区别

| 指标 | 侧重点 | 公式基础 |
|------|--------|----------|
| BLEU | **精确率**：生成的内容有多少是正确的 | Precision |
| ROUGE | **召回率**：参考内容有多少被覆盖了 | Recall |

#### ROUGE 变体

**ROUGE-N**：基于 n-gram 召回率

$$
ROUGE\text{-}N = \frac{\sum_{s \in Reference}\sum_{gram_n \in s} Count_{match}(gram_n)}{\sum_{s \in Reference}\sum_{gram_n \in s} Count(gram_n)}
$$

常用：
- **ROUGE-1**：1-gram 召回
- **ROUGE-2**：2-gram 召回

**ROUGE-L**：基于最长公共子序列（LCS）

$$
ROUGE\text{-}L = \frac{LCS(X, Y)}{|Reference|}
$$

优点：自动考虑句子结构，不需要预定义 n。

**ROUGE-Lsum**：针对多句摘要，逐句计算后求和。

#### 计算示例

**参考摘要**：The quick brown fox jumps over the lazy dog.

**生成摘要**：The fast brown fox leaps over the dog.

| 指标 | 计算 | 得分 |
|------|------|------|
| ROUGE-1 | 匹配词：the, brown, fox, over, the, dog (6/9) | 0.67 |
| ROUGE-2 | 匹配 bigram：brown fox, over the (2/8) | 0.25 |
| ROUGE-L | LCS 长度：6 / 9 | 0.67 |

#### 适用场景

- 文本摘要评估（主要用途）
- 机器翻译（辅助）
- 问答系统（开放式回答）

#### 局限性

- 与 BLEU 类似，无法捕捉语义等价
- 对于高度抽象的摘要效果不佳

---

## 三、语义相似类指标

### 3.1 BERTScore

#### 背景

BERTScore 由 Zhang et al. 于 2019 年提出，利用预训练语言模型的语义理解能力进行评测。

#### 核心思想

使用 BERT 等预训练模型将文本编码为向量，通过**余弦相似度**计算语义相似性，而非简单的词汇匹配。

#### 计算流程

```
参考文本 ──→ BERT编码 ──→ 词向量序列 R = [r₁, r₂, ..., rₘ]
                              ↓
候选文本 ──→ BERT编码 ──→ 词向量序列 C = [c₁, c₂, ..., cₙ]
                              ↓
                    计算相似度矩阵
                              ↓
              Precision / Recall / F1
```

#### 数学定义

**Precision**：

$$
P_{BERT} = \frac{1}{|C|} \sum_{c_i \in C} \max_{r_j \in R} \cos(c_i, r_j)
$$

**Recall**：

$$
R_{BERT} = \frac{1}{|R|} \sum_{r_j \in R} \max_{c_i \in C} \cos(r_j, c_i)
$$

**F1**：

$$
F_{BERT} = 2 \cdot \frac{P_{BERT} \cdot R_{BERT}}{P_{BERT} + R_{BERT}}
$$

#### 优势

| 优势 | 说明 |
|------|------|
| 语义感知 | "happy" 和 "joyful" 被识别为相似 |
| 词序敏感 | 通过上下文编码捕捉词序信息 |
| 多语言 | 使用多语言 BERT 可支持多语言评测 |

#### 局限性

- 计算成本较高（需要 GPU）
- 依赖预训练模型的质量
- 对于专业领域可能需要领域模型

#### 适用场景

- 开放式文本生成评测
- 机器翻译（替代或补充 BLEU）
- 作为其他指标的补充

---

## 四、代码生成类指标

### 4.1 Pass@k

#### 背景

Pass@k 由 OpenAI 在 Codex 论文中提出，专门用于评估代码生成能力。

#### 核心思想

让模型生成 n 个代码样本，如果其中至少 k 个能通过测试用例，则算作通过。

#### 数学定义

$$
Pass@k = \mathbb{E}_{Problems}\left[1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}\right]
$$

其中：
- $n$ 是生成的样本总数
- $c$ 是通过测试的样本数
- $k$ 是需要通过的最少样本数

#### 直观理解

| 指标 | 含义 |
|------|------|
| Pass@1 | 只生成 1 个样本，该样本通过的概率 |
| Pass@10 | 生成 10 个样本，至少 1 个通过的概率 |
| Pass@100 | 生成 100 个样本，至少 1 个通过的概率 |

#### 实际计算

通常采样 n 个样本（如 n=200），然后估算不同 k 值下的通过率。

#### 为什么需要 Pass@k？

代码生成存在随机性，同一个问题多次生成可能得到不同结果。Pass@k 更全面地反映模型能力：

- **Pass@1**：反映"首次即成功"的能力，最严格
- **Pass@10**：反映"多次尝试"的能力，更接近实际使用
- **Pass@100**：反映模型的能力上限

#### 适用场景

- 代码生成模型评测（HumanEval、MBPP）
- 数学推理（生成多个解法）
- 任何可以自动验证的生成任务

---

## 五、语言模型基础指标

### 5.1 Perplexity (困惑度)

#### 定义

Perplexity 衡量语言模型对测试集的"困惑程度"，即模型预测下一个词的不确定性。

$$
PPL = \exp\left(-\frac{1}{N}\sum_{i=1}^{N}\log P(w_i|w_1,...,w_{i-1})\right)
$$

其中 $P(w_i|w_1,...,w_{i-1})$ 是模型预测第 $i$ 个词的概率。

#### 直观理解

- **PPL = 1**：模型完全确定每个词，完美预测
- **PPL = V**（词表大小）：模型完全随机猜测
- **PPL 越低越好**：模型对语言的"理解"越好

#### 示例

假设一个简单句子："I love NLP"

| 位置 | 模型预测概率 |
|------|--------------|
| P("I" \| &lt;start&gt;) | 0.1 |
| P("love" \| "I") | 0.2 |
| P("NLP" \| "I love") | 0.05 |

$$
PPL = \exp\left(-\frac{1}{3}(\log 0.1 + \log 0.2 + \log 0.05)\right) = \exp(3.51) \approx 33.4
$$

#### 适用场景

- 语言模型预训练评估
- 不同模型在相同语料上的比较
- 领域适应性评估

#### 局限性

| 问题 | 说明 |
|------|------|
| 只评估概率 | 不能直接反映任务表现 |
| 依赖分词 | 不同分词方式导致不可比 |
| 不评估生成质量 | 低 PPL 不等于高质量生成 |

---

## 六、指标选择决策框架

### 任务-指标映射

| 任务类型 | 推荐指标 | 备选指标 |
|----------|----------|----------|
| 问答（有标准答案） | Exact Match, F1 | Accuracy |
| 机器翻译 | BLEU-4, COMET | BERTScore |
| 文本摘要 | ROUGE-L, ROUGE-2 | BERTScore |
| 代码生成 | Pass@k | 执行时间、内存 |
| 对话生成 | 人工评估, LLM-as-Judge | BERTScore |
| 分类 | Accuracy, F1, AUC | Precision, Recall |

### 多指标组合

实际评测通常结合多个指标：

```
综合评估 = {
    "自动指标": [BLEU, ROUGE, BERTScore],
    "执行指标": [Pass@k, 成功率],
    "人工评估": [流畅性, 相关性, 有用性]
}
```

---

## 本节小结

1. **精确匹配类**（EM、Accuracy）：适合有标准答案的任务，简单直接但对格式敏感
2. **文本重叠类**（BLEU、ROUGE）：基于 n-gram 匹配，适合翻译和摘要，但忽略语义
3. **语义相似类**（BERTScore）：利用预训练模型捕捉语义，计算成本较高
4. **代码执行类**（Pass@k）：通过执行验证正确性，是代码生成的标准指标
5. **语言模型类**（Perplexity）：评估模型基础能力，不直接反映任务表现

选择指标时需考虑：**任务特性、评测成本、可解释性、与人类判断的相关性**。

## 思考题

1. 为什么 BLEU 使用精确率而 ROUGE 使用召回率？这与它们的原始应用场景有什么关系？
2. 如果要评估一个客服对话机器人，你会选择哪些指标？为什么？
3. Pass@100 很高但 Pass@1 很低意味着什么？这对产品设计有什么启示？
