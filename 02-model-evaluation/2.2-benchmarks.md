# 2.2 主流 Benchmark

## 知识与推理类

### MMLU (Massive Multitask Language Understanding)
- **规模**：57 学科，14k 题目
- **形式**：4 选 1 多选题
- **覆盖**：STEM、人文、社科等
- **现状**：已饱和（GPT-4 > 90%），区分度下降

### MMLU-Pro
- **改进**：MMLU 的升级版
- **规模**：14 领域，更难的推理题
- **形式**：10 选 1（增加干扰项）
- **地位**：当前主流评测标准

### GSM8K
- **内容**：小学数学应用题
- **规模**：8.5k 道题
- **考察**：多步推理能力
- **现状**：顶级模型已超过 95%

### MATH
- **内容**：竞赛级数学题
- **难度**：远高于 GSM8K
- **覆盖**：代数、几何、数论等
- **意义**：区分模型的深度推理能力

## 代码生成类

### HumanEval
- **来源**：OpenAI 发布
- **规模**：164 道 Python 编程题
- **评估**：Pass@k 指标
- **局限**：题目相对简单，存在数据污染风险

### MBPP (Mostly Basic Python Problems)
- **规模**：974 道题
- **特点**：比 HumanEval 更基础
- **用途**：与 HumanEval 互补

## 常识推理类

### HellaSwag
- **任务**：句子补全（常识推理）
- **形式**：给定场景，选择最合理的后续
- **难点**：需要理解日常生活常识

### TruthfulQA
- **目标**：检测模型幻觉
- **内容**：设计诱导错误回答的问题
- **意义**：评估模型的可靠性
