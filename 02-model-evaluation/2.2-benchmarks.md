# 2.2 主流 Benchmark

## 概述

Benchmark（基准测试）是用于评估模型能力的标准化测试集。一个好的 Benchmark 应该具备以下特性：

- **代表性**：能够反映真实场景中的任务需求
- **区分度**：能够区分不同水平的模型
- **可复现**：标准化的评测流程，确保结果可比
- **公平性**：避免数据泄露和过拟合

本节将介绍 LLM 评测中最具影响力的 Benchmark，包括其设计理念、数据构成、评测方法和局限性。

## Benchmark 分类体系

| 类别 | 评测目标 | 代表 Benchmark |
|------|----------|----------------|
| 知识理解 | 世界知识掌握程度 | MMLU, MMLU-Pro |
| 数学推理 | 数学问题解决能力 | GSM8K, MATH |
| 代码生成 | 编程能力 | HumanEval, MBPP |
| 常识推理 | 日常常识理解 | HellaSwag, WinoGrande |
| 真实性 | 抗幻觉能力 | TruthfulQA |
| 综合能力 | 多维度综合 | HELM, AlpacaEval |

---

## 一、知识理解类 Benchmark

### 1.1 MMLU (Massive Multitask Language Understanding)

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2020 年 |
| 发布机构 | UC Berkeley |
| 论文 | Measuring Massive Multitask Language Understanding |
| 数据规模 | 约 14,000 道题目 |
| 学科覆盖 | 57 个学科 |

#### 设计理念

MMLU 旨在评估语言模型的**广泛知识**和**问题解决能力**，覆盖从小学到专业级别的各类知识。

#### 学科分布

MMLU 的 57 个学科分为四大类：

| 类别 | 学科示例 | 题目占比 |
|------|----------|----------|
| STEM | 数学、物理、计算机、工程 | ~25% |
| 人文 | 历史、哲学、法律 | ~25% |
| 社会科学 | 经济学、心理学、政治学 | ~25% |
| 其他 | 医学、商业、日常知识 | ~25% |

#### 题目格式

标准的四选一多选题：

```
问题：光合作用主要发生在植物细胞的哪个部位？

A. 线粒体
B. 叶绿体
C. 细胞核
D. 核糖体

正确答案：B
```

#### 评测方式

1. **Zero-shot**：直接给问题，不提供示例
2. **Few-shot (5-shot)**：提供 5 个示例后再给问题

#### 代表性得分

| 模型 | MMLU 得分 | 测试时间 |
|------|-----------|----------|
| GPT-4 | 86.4% | 2023.03 |
| Claude 3 Opus | 86.8% | 2024.03 |
| Gemini Ultra | 83.7% | 2023.12 |
| GPT-3.5 | 70.0% | 2022.11 |
| 随机基线 | 25.0% | - |

#### 局限性

| 问题 | 说明 |
|------|------|
| **饱和问题** | 顶级模型已超过 90%，区分度下降 |
| **选项设计** | 干扰项有时过于明显 |
| **知识时效** | 部分题目涉及过时信息 |
| **数据泄露** | 可能已被包含在训练数据中 |

### 1.2 MMLU-Pro

#### 背景

为解决 MMLU 饱和问题，研究者推出了更具挑战性的 MMLU-Pro。

#### 与 MMLU 的区别

| 维度 | MMLU | MMLU-Pro |
|------|------|----------|
| 选项数量 | 4 个 | 10 个 |
| 题目难度 | 中等 | 较难，需要推理 |
| 干扰项质量 | 一般 | 更具迷惑性 |
| 学科数量 | 57 个 | 14 个核心学科 |
| 区分度 | 较低 | 较高 |

#### 设计改进

1. **增加选项**：从 4 选 1 变为 10 选 1，降低猜测概率（从 25% 到 10%）
2. **强化推理**：更多题目需要多步推理，而非简单知识检索
3. **筛选干扰项**：使用 GPT-4 生成更具迷惑性的错误选项

#### 代表性得分

| 模型 | MMLU-Pro 得分 |
|------|---------------|
| GPT-4o | 72.6% |
| Claude 3.5 Sonnet | 78.0% |
| Gemini 1.5 Pro | 75.2% |

---

## 二、数学推理类 Benchmark

### 2.1 GSM8K (Grade School Math 8K)

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2021 年 |
| 发布机构 | OpenAI |
| 数据规模 | 8,500 道题目 |
| 难度级别 | 小学数学应用题 |

#### 设计理念

GSM8K 评估模型的**多步数学推理能力**，题目来源于小学水平的数学应用题，但需要多个推理步骤。

#### 题目示例

```
问题：
小明有 15 个苹果。他给了小红 3 个苹果，然后又买了 7 个苹果。
他现在有多少个苹果？

解答过程：
1. 初始苹果数：15
2. 给小红后：15 - 3 = 12
3. 买了新苹果后：12 + 7 = 19

答案：19 个苹果
```

#### 评测特点

- **需要过程**：不仅要答案正确，推理过程也很重要
- **多步推理**：平均需要 2-8 步推理
- **自然语言**：题目用日常语言描述，需要理解语义

#### 代表性得分

| 模型 | GSM8K 得分 |
|------|------------|
| GPT-4 | 92.0% |
| Claude 3 Opus | 95.0% |
| GPT-3.5 | 57.1% |
| LLaMA-2 70B | 56.8% |

#### 局限性

- **已趋饱和**：顶级模型已超过 95%
- **题目简单**：对于评估高级推理能力不够

### 2.2 MATH

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2021 年 |
| 发布机构 | UC Berkeley |
| 数据规模 | 12,500 道题目 |
| 难度级别 | 竞赛级数学 |

#### 设计理念

MATH 数据集来源于高中数学竞赛（AMC、AIME 等），评估模型解决**复杂数学问题**的能力。

#### 难度分级

MATH 将题目分为 5 个难度级别：

| 级别 | 描述 | 示例 |
|------|------|------|
| Level 1 | 基础 | 简单代数运算 |
| Level 2 | 初级 | 一元方程 |
| Level 3 | 中级 | 二次方程、三角函数 |
| Level 4 | 高级 | 复杂证明、组合 |
| Level 5 | 竞赛 | AIME 级别难题 |

#### 学科分布

| 学科 | 题目数量 |
|------|----------|
| 代数 (Algebra) | 1,744 |
| 数论 (Number Theory) | 869 |
| 几何 (Geometry) | 870 |
| 概率 (Counting & Prob.) | 771 |
| 初等代数 (Prealgebra) | 871 |
| 中级代数 (Intermediate Algebra) | 903 |
| 微积分预备 (Precalculus) | 546 |

#### 题目示例

```
问题：
求满足 x² + y² = 25 且 x + y = 7 的所有实数解 (x, y)。

解答：
由 x + y = 7，得 y = 7 - x
代入 x² + y² = 25：
x² + (7-x)² = 25
x² + 49 - 14x + x² = 25
2x² - 14x + 24 = 0
x² - 7x + 12 = 0
(x-3)(x-4) = 0

解：x = 3, y = 4 或 x = 4, y = 3
答案：(3, 4) 和 (4, 3)
```

#### 代表性得分

| 模型 | MATH 得分 |
|------|-----------|
| GPT-4 | 42.5% |
| Claude 3 Opus | 60.1% |
| GPT-4 + Code Interpreter | 52.9% |

#### 评测方法

- **答案匹配**：使用符号计算验证答案等价性（如 1/2 = 0.5）
- **LaTeX 解析**：支持数学公式格式

---

## 三、代码生成类 Benchmark

### 3.1 HumanEval

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2021 年 |
| 发布机构 | OpenAI |
| 数据规模 | 164 道题目 |
| 编程语言 | Python |

#### 设计理念

HumanEval 通过**手写编程题**评估模型的代码生成能力，每道题都有完整的测试用例。

#### 题目结构

```python
def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """
    Check if in given list of numbers, are any two numbers
    closer to each other than given threshold.

    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
    # 模型需要完成此函数
```

#### 评测方法

1. 模型根据函数签名和文档字符串生成实现
2. 运行预设的测试用例
3. 使用 Pass@k 计算通过率

#### 代表性得分 (Pass@1)

| 模型 | HumanEval Pass@1 |
|------|------------------|
| GPT-4 | 67.0% |
| Claude 3 Opus | 84.9% |
| GPT-3.5 | 48.1% |
| Codex | 28.8% |

#### 局限性

| 问题 | 说明 |
|------|------|
| 规模小 | 仅 164 题，易过拟合 |
| 数据泄露 | 已被广泛使用，可能污染训练集 |
| 单一语言 | 只有 Python |
| 题目简单 | 多为函数级别的简单任务 |

### 3.2 MBPP (Mostly Basic Python Problems)

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2021 年 |
| 发布机构 | Google |
| 数据规模 | 974 道题目 |
| 编程语言 | Python |

#### 与 HumanEval 的比较

| 维度 | HumanEval | MBPP |
|------|-----------|------|
| 题目数量 | 164 | 974 |
| 难度 | 中等 | 较简单 |
| 题目来源 | 手写 | 众包 |
| 测试用例 | 完整 | 相对简单 |

### 3.3 其他代码 Benchmark

| Benchmark | 特点 |
|-----------|------|
| **HumanEval+** | HumanEval 的增强版，更多测试用例 |
| **MultiPL-E** | 多语言版本（18 种编程语言） |
| **DS-1000** | 数据科学代码（NumPy、Pandas 等） |
| **CodeContests** | 竞赛级编程题 |

---

## 四、常识推理类 Benchmark

### 4.1 HellaSwag

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2019 年 |
| 发布机构 | University of Washington |
| 数据规模 | 约 70,000 道题目 |
| 任务类型 | 句子补全（常识推理） |

#### 设计理念

HellaSwag 评估模型的**常识推理能力**，给定一个场景描述，选择最合理的后续发展。

#### 题目示例

```
场景：一个人走进厨房，打开冰箱...

A. 然后开始跳舞
B. 拿出一瓶牛奶准备早餐
C. 发现冰箱里有一只大象
D. 冰箱突然飞向月球

正确答案：B（最符合常识的后续）
```

#### 数据生成方法

使用**对抗过滤**（Adversarial Filtering）：
1. 人类撰写正确答案
2. 语言模型生成干扰项
3. 筛选能骗过模型但人类能辨别的干扰项

#### 代表性得分

| 模型 | HellaSwag 得分 |
|------|----------------|
| GPT-4 | 95.3% |
| Claude 3 Opus | 95.4% |
| 人类 | 95.6% |

### 4.2 WinoGrande

#### 设计理念

基于 Winograd Schema Challenge，评估模型的**代词消解**能力。

#### 题目示例

```
句子：The trophy doesn't fit into the suitcase because it is too [big/small].

问题：什么太大/太小？
- 如果是 "big"：答案是 trophy
- 如果是 "small"：答案是 suitcase
```

这类题目需要理解语义和常识才能正确解答。

---

## 五、真实性评测 Benchmark

### 5.1 TruthfulQA

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2021 年 |
| 发布机构 | Anthropic |
| 数据规模 | 817 道问题 |
| 评测目标 | 检测模型幻觉 |

#### 设计理念

TruthfulQA 专门评估模型是否会生成**虚假但看似合理**的信息。题目设计为诱导模型产生常见误解。

#### 题目类别

| 类别 | 示例问题 |
|------|----------|
| 健康误区 | "每天喝 8 杯水是必须的吗？" |
| 历史谬误 | "长城是从太空唯一可见的建筑吗？" |
| 迷信 | "黑猫会带来厄运吗？" |
| 阴谋论 | "登月是不是假的？" |

#### 评测方式

两个维度：
- **Truthful**：答案是否真实
- **Informative**：答案是否提供了有用信息

#### 代表性得分

| 模型 | TruthfulQA 得分 |
|------|-----------------|
| GPT-4 | 59% |
| Claude 2 | 68% |
| 人类 | 94% |

#### 意义

TruthfulQA 揭示了 LLM 的一个核心问题：**模型可能自信地输出错误信息**。这对于产品设计中的信任感构建非常重要。

---

## 六、Benchmark 饱和与演进

### 饱和问题

随着模型能力提升，早期 Benchmark 逐渐失去区分度：

| Benchmark | 2022 最高分 | 2024 最高分 | 状态 |
|-----------|-------------|-------------|------|
| MMLU | 70% | 90%+ | 趋于饱和 |
| GSM8K | 60% | 95%+ | 已饱和 |
| HellaSwag | 85% | 95%+ | 已饱和 |
| HumanEval | 30% | 90%+ | 趋于饱和 |

### 新一代 Benchmark

为应对饱和问题，研究者开发了更具挑战性的评测：

| 新 Benchmark | 特点 |
|--------------|------|
| MMLU-Pro | 10 选项，更强推理 |
| GPQA | 专家级科学问题 |
| MATH-500 | 竞赛数学子集 |
| SWE-bench | 真实代码修复 |
| GAIA | 复杂多步任务 |

---

## 本节小结

1. **知识类**：MMLU/MMLU-Pro 评估广泛知识，后者区分度更高
2. **数学类**：GSM8K 评估基础推理，MATH 评估高级数学能力
3. **代码类**：HumanEval 是标准基准，但需注意数据泄露问题
4. **常识类**：HellaSwag 等评估日常常识理解
5. **真实性**：TruthfulQA 专门检测模型幻觉

选择 Benchmark 时需考虑：**评测目标、区分度、数据泄露风险、与业务场景的相关性**。

## 思考题

1. 为什么顶级模型在 MMLU 上得分很高，但 TruthfulQA 得分却相对较低？
2. 如果你要评估一个客服机器人，现有的 Benchmark 能满足需求吗？如果不能，你会如何设计评测？
3. Benchmark 饱和后，我们应该如何持续追踪模型能力的提升？
