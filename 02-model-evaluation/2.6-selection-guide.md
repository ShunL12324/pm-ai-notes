# 2.6 模型选型指南

## 概述

模型选型是 AI 产品经理的核心工作之一。面对众多模型选择，如何科学地评估和选择适合业务场景的模型，直接影响产品的质量和成本效益。

本节将建立系统化的模型选型方法论，涵盖选型维度、决策框架和实践建议。

### 选型的核心问题

```
业务需求 → 候选模型 → 评测对比 → 选型决策 → 持续优化
```

关键问题：
1. **需求定义**：我需要模型做什么？
2. **候选筛选**：哪些模型可能满足需求？
3. **评测对比**：如何客观比较候选模型？
4. **决策权衡**：如何平衡能力、成本、风险？
5. **持续优化**：如何应对模型更新和业务变化？

---

## 一、选型维度框架

### 1.1 能力维度

#### 核心能力匹配

| 能力类型 | 评估要点 | 相关 Benchmark |
|----------|----------|----------------|
| **通用理解** | 语言理解、知识广度 | MMLU, C-Eval |
| **数学推理** | 数学问题解决 | GSM8K, MATH |
| **代码生成** | 编程能力 | HumanEval, MBPP |
| **长文本** | 长上下文处理 | LongBench |
| **多模态** | 图文理解 | MMMU, VQA |
| **工具使用** | API 调用、Agent 能力 | ToolBench, AgentBench |

#### 语言支持

| 需求 | 模型选择建议 |
|------|--------------|
| **纯英文** | GPT-4, Claude, Gemini |
| **纯中文** | 通义千问, 文心一言, GLM |
| **中英双语** | GPT-4, Claude, 通义千问 |
| **多语言** | GPT-4, Gemini |

#### 上下文长度

| 场景 | 所需长度 | 模型选择 |
|------|----------|----------|
| 短对话 | 4K-8K | 大多数模型 |
| 文档问答 | 32K-128K | Claude, GPT-4 Turbo |
| 长文本分析 | 128K+ | Claude 3, Gemini 1.5 |
| 代码仓库理解 | 200K+ | Claude 3, Gemini 1.5 Pro |

### 1.2 成本维度

#### API 定价比较（示例，价格可能变化）

| 模型 | 输入价格 ($/M tokens) | 输出价格 ($/M tokens) | 特点 |
|------|----------------------|----------------------|------|
| GPT-4 | $30 | $60 | 能力最强，成本最高 |
| GPT-4 Turbo | $10 | $30 | 性价比提升 |
| GPT-3.5 Turbo | $0.5 | $1.5 | 成本极低 |
| Claude 3 Opus | $15 | $75 | 长文本优势 |
| Claude 3 Sonnet | $3 | $15 | 平衡选择 |
| Claude 3 Haiku | $0.25 | $1.25 | 超低成本 |
| Gemini 1.5 Pro | $7 | $21 | 多模态 |

#### 成本计算示例

```
场景：客服机器人，日均 10,000 次对话

每次对话平均：
- 输入：500 tokens
- 输出：200 tokens

日成本计算（以 GPT-4 Turbo 为例）：
- 输入：10,000 × 500 / 1M × $10 = $50
- 输出：10,000 × 200 / 1M × $30 = $60
- 日成本：$110
- 月成本：$3,300
```

#### 成本优化策略

| 策略 | 说明 | 节省比例 |
|------|------|----------|
| **模型分层** | 简单问题用小模型，复杂问题用大模型 | 50-70% |
| **Prompt 优化** | 减少 token 数量 | 20-40% |
| **缓存** | 缓存常见问题答案 | 30-50% |
| **批处理** | 合并请求降低开销 | 10-20% |

### 1.3 延迟维度

#### 延迟因素

```
总延迟 = 网络延迟 + 首 token 延迟 + 生成延迟

生成延迟 ≈ 输出 token 数 × 每 token 生成时间
```

#### 模型延迟对比

| 模型 | 首 token 延迟 | 生成速度 (tokens/s) |
|------|--------------|---------------------|
| GPT-4 | 1-2s | 20-30 |
| GPT-3.5 Turbo | 0.3-0.5s | 50-80 |
| Claude 3 Opus | 1-2s | 25-35 |
| Claude 3 Haiku | 0.2-0.4s | 80-100 |

#### 场景-延迟要求

| 场景 | 可接受延迟 | 推荐选择 |
|------|-----------|----------|
| 实时对话 | <2s | GPT-3.5, Haiku |
| 代码补全 | <500ms | Haiku, 专用代码模型 |
| 文档分析 | <30s | Opus, GPT-4 |
| 批量处理 | 无限制 | 成本优先 |

### 1.4 部署维度

#### 部署方式对比

| 方式 | 优势 | 劣势 | 适用场景 |
|------|------|------|----------|
| **API 调用** | 零运维、按需付费 | 数据外传、依赖第三方 | 大多数场景 |
| **私有部署** | 数据安全、可定制 | 成本高、运维复杂 | 敏感数据、高并发 |
| **混合部署** | 灵活、安全与成本平衡 | 架构复杂 | 企业级应用 |

#### 私有部署成本估算

| 模型规模 | 所需 GPU | 月成本估算 |
|----------|----------|-----------|
| 7B | 1× A100 40G | $2,000 |
| 13B | 2× A100 40G | $4,000 |
| 70B | 8× A100 80G | $16,000 |

### 1.5 合规维度

#### 数据安全考量

| 考量点 | 说明 |
|--------|------|
| **数据传输** | API 调用是否加密传输 |
| **数据存储** | 提供商是否存储用户数据 |
| **数据使用** | 是否用于模型训练 |
| **地域限制** | 数据是否出境 |

#### 主流提供商数据政策

| 提供商 | API 数据使用政策 |
|--------|------------------|
| OpenAI | 默认不用于训练（API） |
| Anthropic | 不用于训练 |
| Google | 可选择是否用于改进 |
| 国内厂商 | 通常需要签署协议明确 |

---

## 二、主流模型对比

### 2.1 综合能力对比

| 模型 | 推理能力 | 代码能力 | 中文能力 | 长文本 | 多模态 |
|------|----------|----------|----------|--------|--------|
| GPT-4 | ★★★★★ | ★★★★★ | ★★★★ | ★★★★ | ★★★★ |
| Claude 3 Opus | ★★★★★ | ★★★★ | ★★★★ | ★★★★★ | ★★★★ |
| Gemini Ultra | ★★★★ | ★★★★ | ★★★ | ★★★★★ | ★★★★★ |
| 通义千问 Max | ★★★★ | ★★★★ | ★★★★★ | ★★★★ | ★★★★ |
| DeepSeek V2 | ★★★★ | ★★★★★ | ★★★★ | ★★★ | ★★ |

### 2.2 场景-模型推荐

| 场景 | 首选模型 | 备选模型 | 理由 |
|------|----------|----------|------|
| **通用对话** | GPT-4 Turbo | Claude 3 Sonnet | 综合能力强 |
| **客服机器人** | GPT-3.5 Turbo | Claude 3 Haiku | 成本低、速度快 |
| **代码助手** | GPT-4 | DeepSeek Coder | 代码能力强 |
| **文档分析** | Claude 3 Opus | Gemini 1.5 Pro | 长上下文 |
| **中文内容** | 通义千问 | 文心一言 | 中文优化 |
| **图像理解** | GPT-4V | Gemini Pro Vision | 多模态能力 |
| **Agent 系统** | GPT-4 | Claude 3 Opus | 工具使用能力 |

### 2.3 成本-能力矩阵

```
           高能力
              │
    GPT-4     │    Claude 3 Opus
    ($$$)     │    ($$$)
              │
─────────────┼────────────── 高成本
              │
    Claude    │    GPT-4 Turbo
    Sonnet    │    ($$)
    ($$)      │
              │
─────────────┼──────────────
              │
    Claude    │    GPT-3.5
    Haiku     │    ($)
    ($)       │
              │
           低能力
```

---

## 三、选型决策框架

### 3.1 决策流程

```
Step 1: 需求分析
    ↓
Step 2: 候选筛选
    ↓
Step 3: 公开 Benchmark 初筛
    ↓
Step 4: 业务评测集精测
    ↓
Step 5: 成本与风险分析
    ↓
Step 6: 小规模验证
    ↓
Step 7: 选型决策
    ↓
Step 8: 持续监控与优化
```

### 3.2 需求分析清单

#### 功能需求

- [ ] 核心任务类型（对话/生成/分析/代码）
- [ ] 语言要求（中文/英文/多语言）
- [ ] 输入类型（纯文本/图文/多模态）
- [ ] 上下文长度需求
- [ ] 工具使用需求

#### 非功能需求

- [ ] 延迟要求（实时/准实时/异步）
- [ ] 并发量预估
- [ ] 可用性要求（SLA）
- [ ] 预算约束

#### 合规需求

- [ ] 数据安全等级
- [ ] 数据出境限制
- [ ] 行业监管要求
- [ ] 审计追溯需求

### 3.3 评测设计

#### 建立业务评测集

```
评测集构成：
├── 核心场景（60%）：日常高频场景
├── 边界场景（25%）：特殊情况、异常输入
└── 困难场景（15%）：已知的难题
```

#### 评测维度

| 维度 | 权重（示例）| 评估方法 |
|------|------------|----------|
| 准确性 | 40% | 人工评分/自动指标 |
| 相关性 | 20% | 人工评分 |
| 安全性 | 15% | 安全测试用例 |
| 流畅性 | 10% | LLM-as-Judge |
| 格式规范 | 10% | 规则检查 |
| 延迟 | 5% | 自动测量 |

#### 评测报告模板

```
## 模型评测报告

### 基本信息
- 评测日期：2024-XX-XX
- 评测模型：GPT-4 Turbo vs Claude 3 Sonnet
- 评测样本：500 条

### 整体得分
| 模型 | 综合得分 | 准确性 | 安全性 | 延迟 |
|------|----------|--------|--------|------|
| GPT-4 Turbo | 85.2 | 87% | 92% | 1.5s |
| Claude 3 Sonnet | 83.5 | 84% | 95% | 1.2s |

### 场景分析
[详细分场景对比]

### 典型错误案例
[失败案例分析]

### 结论与建议
[选型建议]
```

### 3.4 决策矩阵

使用加权评分做最终决策：

| 评估维度 | 权重 | 模型 A | 模型 B | 模型 C |
|----------|------|--------|--------|--------|
| 准确性 | 30% | 85 | 80 | 75 |
| 成本 | 25% | 60 | 80 | 90 |
| 延迟 | 15% | 70 | 85 | 90 |
| 安全性 | 15% | 90 | 95 | 85 |
| 生态支持 | 15% | 95 | 85 | 70 |
| **加权总分** | 100% | **78.5** | **83.5** | **82.0** |

---

## 四、实践建议

### 4.1 避免常见陷阱

#### 陷阱 1：过度依赖公开 Benchmark

```
问题：模型在 MMLU 上 90 分，在业务场景却表现不佳
原因：Benchmark 与业务场景不匹配
解决：必须用业务数据评测
```

#### 陷阱 2：只看平均表现

```
问题：模型平均准确率 85%，但某些场景 <50%
原因：平均值掩盖了 worst case
解决：分场景评测，关注最差表现
```

#### 陷阱 3：忽视成本随规模变化

```
问题：测试阶段成本可控，上线后成本爆炸
原因：未考虑规模效应
解决：早期做成本测算，设计分层策略
```

#### 陷阱 4：低估模型更新影响

```
问题：模型更新后效果下降或行为改变
原因：模型提供商持续更新
解决：建立回归测试机制，评估更新影响
```

### 4.2 模型分层策略

对于复杂应用，采用多模型分层：

```
用户请求
    ↓
路由判断（规则/小模型）
    ↓
┌───────┼───────┐
↓       ↓       ↓
简单    中等    复杂
GPT-3.5 Sonnet  GPT-4
(70%)   (20%)   (10%)
```

#### 分层效果

| 策略 | 成本 | 质量 |
|------|------|------|
| 全用 GPT-4 | $100 | 95% |
| 分层策略 | $35 | 93% |
| 全用 GPT-3.5 | $10 | 80% |

### 4.3 持续优化机制

#### 监控指标

| 指标 | 监控方式 | 告警阈值 |
|------|----------|----------|
| 准确率 | 人工抽检 + 自动评测 | <85% |
| 用户满意度 | 用户反馈 | <4.0/5.0 |
| 延迟 P99 | 自动监控 | >5s |
| 成本/请求 | 日报表 | >预算 20% |

#### 优化周期

```
日常：监控核心指标
每周：分析失败案例
每月：更新评测集，重新评测
每季：评估新模型，考虑迁移
```

### 4.4 风险管理

#### 供应商风险

| 风险 | 缓解措施 |
|------|----------|
| 服务中断 | 备用模型、降级策略 |
| 价格上涨 | 多供应商、私有部署备选 |
| 政策变化 | 数据主权规划 |

#### 技术风险

| 风险 | 缓解措施 |
|------|----------|
| 模型更新回退 | 回归测试、版本锁定 |
| 幻觉/错误 | 人工审核、Guardrails |
| 安全漏洞 | 输入输出过滤、监控 |

---

## 五、案例分析

### 案例 1：电商客服机器人

**需求**：
- 日均 50,000 次对话
- 中文为主
- 延迟 <3s
- 预算 $3,000/月

**选型过程**：

1. **候选筛选**：GPT-4、GPT-3.5、通义千问、Claude 3 Haiku
2. **成本筛选**：GPT-4 成本过高，排除
3. **中文评测**：通义千问 > GPT-3.5 > Haiku
4. **最终选择**：通义千问（中文优势）+ GPT-3.5（备选）

**分层策略**：
- 常见问题：通义千问（80%）
- 复杂问题：GPT-3.5 Turbo（15%）
- 人工转接：（5%）

### 案例 2：代码审查助手

**需求**：
- 多语言代码（Python、Java、Go）
- 安全漏洞检测
- 代码质量建议
- 私有代码，数据敏感

**选型过程**：

1. **代码能力评测**：GPT-4 > DeepSeek Coder > Claude 3
2. **安全考量**：需要私有部署
3. **私有部署成本**：DeepSeek 更经济
4. **最终选择**：DeepSeek Coder 私有部署 + GPT-4 API 备选

---

## 本节小结

1. **选型维度**：能力、成本、延迟、部署、合规五大维度
2. **决策框架**：需求分析 → 候选筛选 → 评测对比 → 风险分析 → 决策
3. **评测设计**：结合公开 Benchmark 和业务评测集
4. **优化策略**：模型分层、持续监控、定期优化
5. **风险管理**：供应商分散、回归测试、降级预案

模型选型没有标准答案，需要根据具体业务需求权衡取舍。

## 思考题

1. 如果预算有限但对质量要求高，你会如何设计模型分层策略？
2. 如何建立一套机制，让团队能够快速评估和响应新模型的发布？
3. 在选择私有部署 vs API 调用时，除了成本和安全，还应该考虑哪些因素？
