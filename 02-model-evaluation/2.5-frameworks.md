# 2.5 评测框架

## 概述

评测框架是执行模型评测的工具平台，提供标准化的评测流程、丰富的任务集成和可复现的评测环境。选择合适的评测框架可以大幅提高评测效率和结果可信度。

### 评测框架的核心功能

| 功能 | 说明 |
|------|------|
| **任务管理** | 内置大量评测任务，支持自定义 |
| **模型接入** | 支持多种模型格式和推理后端 |
| **指标计算** | 自动计算各类评测指标 |
| **结果报告** | 生成结构化的评测报告 |
| **可复现性** | 标准化流程确保结果可复现 |

---

## 一、lm-evaluation-harness

### 基本信息

| 属性 | 内容 |
|------|------|
| 开发者 | EleutherAI |
| 开源地址 | github.com/EleutherAI/lm-evaluation-harness |
| 编程语言 | Python |
| Star 数 | 5,000+ |
| 地位 | 最广泛使用的开源评测框架 |

### 特点

#### 优势

| 特点 | 说明 |
|------|------|
| **任务丰富** | 支持 200+ 评测任务 |
| **模型支持广** | HuggingFace、vLLM、GGUF 等多种格式 |
| **易于扩展** | 简单的 YAML 配置即可添加新任务 |
| **社区活跃** | 持续更新，bug 修复快 |
| **标准化** | 成为开源模型评测的事实标准 |

#### 局限

- 主要聚焦英文任务
- Agent 评测支持有限
- 部分任务的 prompt 模板可能影响结果

### 支持的评测任务

| 类别 | 代表任务 |
|------|----------|
| **知识理解** | MMLU, ARC, HellaSwag |
| **数学推理** | GSM8K, MATH |
| **代码生成** | HumanEval, MBPP |
| **阅读理解** | SQuAD, TriviaQA |
| **常识推理** | WinoGrande, PIQA |
| **真实性** | TruthfulQA |

### 使用示例

#### 安装

```bash
pip install lm-eval
```

#### 基本使用

```bash
# 评测 HuggingFace 模型
lm_eval --model hf \
    --model_args pretrained=meta-llama/Llama-2-7b-hf \
    --tasks mmlu,hellaswag,arc_easy \
    --batch_size 8 \
    --output_path ./results
```

#### Python API

```python
from lm_eval import evaluator
from lm_eval.models.huggingface import HFLM

# 加载模型
model = HFLM(pretrained="meta-llama/Llama-2-7b-hf")

# 运行评测
results = evaluator.simple_evaluate(
    model=model,
    tasks=["mmlu", "hellaswag"],
    batch_size=8
)

print(results["results"])
```

#### 输出示例

```json
{
  "results": {
    "mmlu": {
      "acc": 0.456,
      "acc_stderr": 0.012
    },
    "hellaswag": {
      "acc_norm": 0.782,
      "acc_norm_stderr": 0.008
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=meta-llama/Llama-2-7b-hf"
  }
}
```

### 自定义任务

通过 YAML 配置添加新任务：

```yaml
# my_task.yaml
task: my_custom_task
dataset_path: my_dataset
dataset_name: default
output_type: multiple_choice
doc_to_text: "Question: {{question}}\nAnswer:"
doc_to_target: "{{answer}}"
metric_list:
  - metric: acc
    aggregation: mean
    higher_is_better: true
```

---

## 二、OpenCompass

### 基本信息

| 属性 | 内容 |
|------|------|
| 开发者 | 上海人工智能实验室 |
| 开源地址 | github.com/open-compass/opencompass |
| 官网 | opencompass.org.cn |
| 编程语言 | Python |
| 特色 | 中英文双语、国产模型支持 |

### 特点

#### 优势

| 特点 | 说明 |
|------|------|
| **中文支持强** | 丰富的中文评测任务 |
| **国产模型** | 对智谱、百川、通义等支持良好 |
| **在线排行榜** | 提供公开的模型排名 |
| **评测全面** | 覆盖能力、安全、长文本等多维度 |
| **分布式** | 支持分布式评测加速 |

#### 局限

- 国际社区认知度相对较低
- 部分英文任务覆盖不如 lm-eval-harness

### 评测维度

OpenCompass 提供多维度评测体系：

| 维度 | 子维度 | 代表任务 |
|------|--------|----------|
| **语言能力** | 语言理解、生成 | C-Eval, CMMLU |
| **知识能力** | 世界知识、专业知识 | MMLU, AGIEval |
| **推理能力** | 数学、逻辑、代码 | GSM8K, HumanEval |
| **安全能力** | 有害性、偏见 | SafetyBench |
| **长文本** | 长上下文理解 | LongBench |

### 支持的中文任务

| 任务 | 说明 |
|------|------|
| **C-Eval** | 中文综合能力评测 |
| **CMMLU** | 中文版 MMLU |
| **GAOKAO** | 高考题目 |
| **AGIEval** | 通用人工智能评测 |
| **CLUE** | 中文语言理解 |
| **BBH-CN** | 中文版 Big-Bench Hard |

### 使用示例

#### 安装

```bash
git clone https://github.com/open-compass/opencompass.git
cd opencompass
pip install -e .
```

#### 配置文件

```python
# configs/eval_demo.py
from mmengine.config import read_base

with read_base():
    from .datasets.ceval.ceval_gen import ceval_datasets
    from .models.hf_llama.hf_llama2_7b import models

datasets = ceval_datasets
```

#### 运行评测

```bash
python run.py configs/eval_demo.py
```

### 排行榜

OpenCompass 维护公开的模型排行榜：

| 维度 | 说明 |
|------|------|
| **综合排行榜** | 多维度综合得分 |
| **中文排行榜** | 中文能力专项 |
| **代码排行榜** | 代码生成能力 |
| **Agent 排行榜** | Agent 任务能力 |

---

## 三、HELM (Holistic Evaluation of Language Models)

### 基本信息

| 属性 | 内容 |
|------|------|
| 开发者 | Stanford CRFM |
| 官网 | crfm.stanford.edu/helm |
| 论文 | Holistic Evaluation of Language Models |
| 特色 | 强调全面性和多维度评估 |

### 设计理念

HELM 强调"全面性"（Holistic），不仅评估准确性，还关注：

```
传统评测：准确性
    ↓
HELM：准确性 + 校准性 + 鲁棒性 + 公平性 + 效率 + ...
```

### 评估维度

| 维度 | 说明 | 指标示例 |
|------|------|----------|
| **准确性** | 任务完成质量 | Accuracy, F1 |
| **校准性** | 置信度与正确率的一致性 | ECE |
| **鲁棒性** | 对扰动的稳定性 | 扰动后性能下降 |
| **公平性** | 不同群体的表现差异 | 群体间性能差距 |
| **偏见** | 有害内容和刻板印象 | 偏见检测得分 |
| **毒性** | 生成有害内容的倾向 | 毒性检测得分 |
| **效率** | 推理速度和资源消耗 | Tokens/秒 |

### 任务分类

HELM 将任务组织为多个场景：

| 场景 | 任务示例 |
|------|----------|
| **问答** | NaturalQuestions, TriviaQA |
| **信息检索** | MS MARCO |
| **摘要** | CNN/DailyMail, XSum |
| **情感分析** | IMDB, SST |
| **有害内容** | RealToxicityPrompts |
| **版权** | 记忆化检测 |

### 特色功能

#### 多模型对比

HELM 提供详细的模型对比分析：

```
模型 A vs 模型 B
├── 准确性：A > B (+5%)
├── 鲁棒性：A < B (-3%)
├── 公平性：A ≈ B
└── 效率：A < B (2x slower)
```

#### 场景适配

针对不同应用场景推荐模型：

| 场景 | 推荐考虑 |
|------|----------|
| **客服** | 准确性 + 安全性 |
| **代码助手** | 代码能力 + 效率 |
| **医疗问答** | 准确性 + 校准性 |

### 使用示例

```bash
# 安装
pip install crfm-helm

# 运行评测
helm-run \
    --run-entries "mmlu:model=openai/gpt-4" \
    --suite my_suite \
    --max-eval-instances 100
```

---

## 四、Ragas

### 基本信息

| 属性 | 内容 |
|------|------|
| 开发者 | Exploding Gradients |
| 开源地址 | github.com/explodinggradients/ragas |
| 定位 | RAG 系统专用评测 |
| 特色 | 轻量级、可集成 CI/CD |

### 设计理念

Ragas 专注于评估 **RAG（Retrieval-Augmented Generation）** 系统，覆盖检索和生成两个环节。

### 评估维度

| 维度 | 评估对象 | 指标 |
|------|----------|------|
| **上下文精确度** | 检索结果 | Context Precision |
| **上下文召回** | 检索结果 | Context Recall |
| **忠实度** | 生成质量 | Faithfulness |
| **答案相关性** | 生成质量 | Answer Relevancy |

### 指标详解

#### Faithfulness（忠实度）

评估生成答案是否忠实于检索到的上下文：

```
检索内容：北京是中国的首都，有悠久的历史。
生成答案：北京是中国的首都，是一座现代化大都市。

忠实度分析：
- "北京是中国的首都" ✓ 基于上下文
- "现代化大都市" ✗ 上下文未提及

忠实度得分：0.5
```

#### Answer Relevancy（答案相关性）

评估答案是否回答了用户问题：

```
问题：北京有什么著名景点？
答案：北京是中国的首都。

相关性：低（未回答问题）
```

### 使用示例

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision

# 准备数据
data = {
    "question": ["北京有什么著名景点？"],
    "answer": ["北京有故宫、长城、天坛等著名景点。"],
    "contexts": [["北京著名景点包括故宫、长城、天坛、颐和园等。"]],
    "ground_truth": ["故宫、长城、天坛"]
}

# 运行评测
result = evaluate(
    dataset=data,
    metrics=[faithfulness, answer_relevancy, context_precision]
)

print(result)
```

### 集成 CI/CD

Ragas 可以集成到持续集成流程：

```yaml
# .github/workflows/rag-eval.yml
name: RAG Evaluation

on: [push]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run Ragas Evaluation
        run: |
          pip install ragas
          python evaluate_rag.py
      - name: Check Score Threshold
        run: |
          if [ $(cat score.txt) -lt 0.8 ]; then
            exit 1
          fi
```

---

## 五、其他评测工具

### 5.1 LangSmith

| 属性 | 内容 |
|------|------|
| 开发者 | LangChain |
| 定位 | LLM 应用全生命周期管理 |
| 特色 | 追踪、评测、调试一体化 |

功能：
- 请求追踪和日志
- A/B 测试
- 人工标注平台
- 自动评测集成

### 5.2 Promptfoo

| 属性 | 内容 |
|------|------|
| 开发者 | 开源社区 |
| 定位 | Prompt 评测和测试 |
| 特色 | 轻量、本地运行 |

适用场景：
- Prompt 版本对比
- 回归测试
- CI/CD 集成

### 5.3 DeepEval

| 属性 | 内容 |
|------|------|
| 开发者 | Confident AI |
| 定位 | LLM 单元测试 |
| 特色 | 类似 pytest 的使用体验 |

```python
from deepeval import assert_test
from deepeval.test_case import LLMTestCase

def test_summarization():
    test_case = LLMTestCase(
        input="长文本...",
        actual_output="摘要...",
        expected_output="参考摘要..."
    )
    assert_test(test_case, metrics=[summarization_metric])
```

---

## 六、框架选择指南

### 场景-框架映射

| 使用场景 | 推荐框架 | 原因 |
|----------|----------|------|
| **开源模型评测** | lm-evaluation-harness | 任务最全、社区认可 |
| **国产模型/中文** | OpenCompass | 中文任务丰富 |
| **学术研究** | HELM | 多维度全面评估 |
| **RAG 系统** | Ragas | 专门针对 RAG |
| **产品迭代** | LangSmith + Promptfoo | 追踪 + 回归测试 |
| **LLM 单元测试** | DeepEval | 开发者友好 |

### 决策流程

```
需要评测什么？
│
├── 开源基座模型 → lm-eval-harness / OpenCompass
│
├── RAG 系统 → Ragas
│
├── Agent 系统 → AgentBench / 自定义评测
│
├── Prompt 版本 → Promptfoo
│
└── 生产环境监控 → LangSmith
```

### 组合使用

实际项目中常组合使用多个框架：

```
开发阶段：
├── lm-eval-harness：选择基座模型
├── Promptfoo：优化 prompt
└── Ragas：评测 RAG pipeline

生产阶段：
├── LangSmith：监控和追踪
├── DeepEval：回归测试
└── 人工评测：定期抽检
```

---

## 本节小结

1. **lm-evaluation-harness**：开源模型评测的事实标准，任务丰富
2. **OpenCompass**：国产框架，中文任务覆盖全面
3. **HELM**：Stanford 出品，强调多维度全面评估
4. **Ragas**：RAG 系统专用，可集成 CI/CD
5. **其他工具**：LangSmith（全生命周期）、Promptfoo（prompt 测试）、DeepEval（单元测试）

选择框架时需考虑：**任务类型、模型格式、评测维度、团队技术栈**。

## 思考题

1. 如果你要评测一个中英双语的对话模型，会选择哪些框架组合？为什么？
2. HELM 强调"全面性"评估，这对产品决策有什么帮助？有什么局限？
3. 如何设计一个评测流程，既能快速迭代又能保证评测质量？
