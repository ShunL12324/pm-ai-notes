# 2.3 Agent 评测 Benchmark

Agent 评测关注模型在真实环境中的任务执行能力，而非单纯的问答。

## AgentBench

- **定位**：综合性 Agent 评测
- **环境**：8 种不同环境
  - 操作系统 (OS)
  - 数据库 (DB)
  - 网页浏览 (Web)
  - 知识图谱
  - 等...
- **评估**：任务完成率、步骤效率
- **意义**：最全面的 Agent 能力评测

## SWE-bench

- **定位**：软件工程能力评测
- **内容**：真实 GitHub Issue 修复
- **特点**：
  - 来自真实开源项目
  - Docker 隔离保证可复现
  - 需要理解代码库上下文
- **难度**：顶级模型 < 50%
- **变体**：SWE-bench Lite（更小规模）

## WebArena

- **定位**：网页操作能力
- **环境**：模拟真实网站
  - 电商、论坛、地图等
- **任务**：完成具体操作目标
- **评估**：任务成功率
- **挑战**：需要理解 UI、执行多步操作

## GAIA

- **定位**：通用 AI 助手评测
- **特点**：
  - 需要工具使用
  - 支持多模态（图片、文件）
- **难度**：3 级递进
  - Level 1：简单工具调用
  - Level 2：多步推理
  - Level 3：复杂任务规划
- **意义**：贴近真实助手使用场景

## ToolBench

- **定位**：API 调用能力
- **规模**：16,000+ 真实 API
- **评估**：
  - API 选择准确率
  - 参数填充正确率
  - 任务完成率
- **应用**：评估模型的工具使用能力
