# 2.3 Agent 评测 Benchmark

## 概述

随着 LLM 能力的增强，研究者开始探索让模型作为**智能代理（Agent）**执行复杂任务。Agent 不仅需要理解自然语言，还需要：

- **规划**：将复杂任务分解为子任务
- **工具使用**：调用 API、执行代码、浏览网页
- **环境交互**：观察反馈、调整策略
- **长程推理**：跨多个步骤保持目标一致性

传统 Benchmark 难以评估这些复合能力，因此催生了专门的 Agent 评测基准。

## Agent 评测的特殊性

### 与传统评测的区别

| 维度 | 传统 LLM 评测 | Agent 评测 |
|------|---------------|------------|
| 交互模式 | 单轮输入-输出 | 多轮交互 |
| 评测对象 | 模型回复 | 任务完成度 |
| 环境依赖 | 无 | 需要模拟/真实环境 |
| 工具使用 | 无或有限 | 核心能力 |
| 评测成本 | 较低 | 较高 |
| 确定性 | 较高 | 较低（路径多样） |

### Agent 能力分解

一个完整的 Agent 系统需要以下能力：

```
Agent 能力金字塔
┌─────────────────────────────┐
│     任务完成 (Goal Achievement)     │ ← 最终目标
├─────────────────────────────┤
│   规划与决策 (Planning & Decision)  │
├─────────────────────────────┤
│   工具调用 (Tool Use)              │
├─────────────────────────────┤
│   环境理解 (Environment Understanding)│
├─────────────────────────────┤
│   基础语言能力 (Language)           │ ← 基础
└─────────────────────────────┘
```

---

## 一、通用 Agent Benchmark

### 1.1 AgentBench

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2023 年 |
| 发布机构 | 清华大学 |
| 论文 | AgentBench: Evaluating LLMs as Agents |
| 环境数量 | 8 个不同环境 |
| 任务数量 | 数千个任务 |

#### 设计理念

AgentBench 是首个系统性评估 LLM 作为 Agent 能力的综合基准，覆盖多种真实场景。

#### 环境分类

AgentBench 包含 8 个评测环境，分为三大类：

**代码类环境**

| 环境 | 任务描述 |
|------|----------|
| **OS (操作系统)** | 在 Linux 终端中执行系统管理任务 |
| **DB (数据库)** | 编写 SQL 查询操作数据库 |
| **KG (知识图谱)** | 查询知识图谱获取信息 |

**游戏类环境**

| 环境 | 任务描述 |
|------|----------|
| **LTP (Lateral Thinking)** | 水平思维谜题（如海龟汤） |
| **HouseHold** | ALFWorld 家务任务 |
| **WebShop** | 网上购物模拟 |

**网络类环境**

| 环境 | 任务描述 |
|------|----------|
| **Mind2Web** | 真实网页交互 |
| **Card Game** | 扑克牌游戏策略 |

#### 评测流程

```
任务描述 → Agent 生成动作 → 环境执行 → 返回观察
    ↑                                    ↓
    └──────────── 循环直到完成或超时 ←────┘
```

每个环境有特定的成功判定标准：
- **OS**：检查文件/系统状态是否符合预期
- **DB**：验证查询结果正确性
- **WebShop**：商品匹配度和价格

#### 代表性结果

| 模型 | 总分 | OS | DB | KG |
|------|------|----|----|-----|
| GPT-4 | 4.01 | 42.4% | 32.5% | 48.3% |
| Claude 2 | 2.49 | 31.2% | 21.8% | 35.6% |
| GPT-3.5 | 1.86 | 24.5% | 15.3% | 28.4% |

#### 关键发现

1. **能力差距大**：顶级模型与基线相比有显著优势
2. **环境敏感**：同一模型在不同环境表现差异大
3. **长程任务难**：步骤越多，成功率下降越明显

### 1.2 GAIA (General AI Assistants)

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2023 年 |
| 发布机构 | Meta AI |
| 任务数量 | 466 道 |
| 难度级别 | 3 级 |

#### 设计理念

GAIA 关注真实世界中人类能够快速解决、但 AI 需要复杂推理和工具使用的问题。其核心理念是：

> "简单对人类，困难对 AI"

#### 任务特点

GAIA 任务需要综合以下能力：

| 能力 | 示例 |
|------|------|
| **网络搜索** | 查找特定信息 |
| **文件处理** | 读取 PDF、Excel |
| **计算** | 数学运算 |
| **多步推理** | 整合多个信息源 |

#### 难度分级

| 级别 | 描述 | 典型步骤数 |
|------|------|-----------|
| Level 1 | 简单查询，1-2 个工具 | 1-3 步 |
| Level 2 | 中等复杂，多工具组合 | 4-7 步 |
| Level 3 | 困难任务，复杂推理 | 8+ 步 |

#### 题目示例

**Level 1 示例**：
```
问题：2023年诺贝尔物理学奖得主中，哪位来自法国？

需要能力：网络搜索 + 信息提取
```

**Level 3 示例**：
```
问题：在附件的 Excel 文件中，计算所有"东区"销售额的
总和，并找出销售额最高的产品类别。

需要能力：文件读取 + 数据过滤 + 聚合计算 + 排序
```

#### 评测结果

| 模型 | Level 1 | Level 2 | Level 3 | 平均 |
|------|---------|---------|---------|------|
| GPT-4 + Plugins | 54.3% | 31.2% | 7.8% | 30.4% |
| 人类 | 92.0% | 88.5% | 85.2% | 88.6% |

#### 意义

GAIA 揭示了当前 AI 系统与人类在**实际问题解决**能力上的巨大差距，为 Agent 研究提供了清晰的改进方向。

---

## 二、代码 Agent Benchmark

### 2.1 SWE-bench

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2023 年 |
| 发布机构 | Princeton 大学 |
| 数据来源 | 真实 GitHub Issue |
| 任务数量 | 2,294 个问题 |
| 涉及项目 | 12 个 Python 开源项目 |

#### 设计理念

SWE-bench 评估 Agent 解决**真实软件工程问题**的能力。与 HumanEval 等人工设计题目不同，SWE-bench 使用真实的 GitHub Issue 和对应的 Pull Request。

#### 数据来源项目

| 项目 | 领域 | 问题数量 |
|------|------|----------|
| Django | Web 框架 | 571 |
| Flask | Web 框架 | 123 |
| Matplotlib | 可视化 | 234 |
| NumPy | 数值计算 | 156 |
| Pandas | 数据处理 | 287 |
| Scikit-learn | 机器学习 | 198 |
| Requests | HTTP 库 | 89 |
| ... | ... | ... |

#### 任务格式

```
输入：
- Issue 描述（用户报告的 bug 或功能需求）
- 完整代码仓库

输出：
- 修复 Issue 的代码补丁（patch）

评测：
- 运行项目的测试用例
- 检查是否通过相关测试
```

#### 评测流程

```
1. Agent 接收 Issue 描述
         ↓
2. 探索代码仓库（定位相关文件）
         ↓
3. 理解代码逻辑
         ↓
4. 生成修复补丁
         ↓
5. 运行测试验证
```

#### 代表性结果

| 模型/系统 | 解决率 |
|-----------|--------|
| Claude 3 Opus | 4.3% |
| GPT-4 | 1.7% |
| SWE-Agent + GPT-4 | 12.5% |
| Devin (声称) | 13.8% |
| 人类开发者 | ~100% |

#### SWE-bench 变体

为降低评测成本，发布了精简版：

| 版本 | 任务数量 | 特点 |
|------|----------|------|
| SWE-bench | 2,294 | 完整版 |
| SWE-bench Lite | 300 | 筛选独立、可验证的问题 |
| SWE-bench Verified | 500 | 人工验证，更高质量 |

#### 关键挑战

| 挑战 | 说明 |
|------|------|
| **代码导航** | 在大型仓库中定位相关代码 |
| **上下文理解** | 理解代码依赖和调用关系 |
| **修复精确性** | 生成语法正确且逻辑正确的补丁 |
| **测试兼容** | 修复不能破坏已有功能 |

### 2.2 HumanEval-X

#### 设计理念

HumanEval-X 将 HumanEval 扩展到多种编程语言，评估 Agent 的**跨语言代码能力**。

#### 支持语言

| 语言 | 题目数量 |
|------|----------|
| Python | 164 |
| Java | 164 |
| JavaScript | 164 |
| C++ | 164 |
| Go | 164 |

#### 评测模式

- **代码生成**：根据描述生成指定语言的代码
- **代码翻译**：将代码从一种语言翻译到另一种

---

## 三、Web Agent Benchmark

### 3.1 WebArena

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2023 年 |
| 发布机构 | CMU |
| 网站类型 | 5 类真实网站 |
| 任务数量 | 812 个任务 |

#### 设计理念

WebArena 提供**真实网站环境**，评估 Agent 在网页上完成任务的能力。

#### 网站环境

| 网站类型 | 模拟原型 | 任务示例 |
|----------|----------|----------|
| 电商 | Amazon | 搜索商品、下单、查看订单 |
| 论坛 | Reddit | 发帖、回复、搜索 |
| 内容管理 | GitLab | 创建项目、提交代码 |
| 地图 | Google Maps | 查找路线、搜索地点 |
| 知识库 | Wikipedia | 信息检索、编辑 |

#### 观察与动作空间

**观察空间**：
- HTML DOM 树
- 页面截图
- 可访问性树（Accessibility Tree）

**动作空间**：

| 动作 | 描述 | 示例 |
|------|------|------|
| `click(element)` | 点击元素 | click("add to cart") |
| `type(element, text)` | 输入文本 | type("search", "laptop") |
| `scroll(direction)` | 滚动页面 | scroll("down") |
| `goto(url)` | 导航到 URL | goto("cart") |
| `back()` | 返回上一页 | back() |

#### 任务示例

```
任务：在电商网站上购买一台价格低于 500 美元的笔记本电脑，
     要求内存至少 8GB，并使用优惠券 "SAVE10"。

步骤序列：
1. goto(homepage)
2. type(search_box, "laptop")
3. click(search_button)
4. click(filter_price)
5. type(max_price, "500")
6. click(filter_memory)
7. click("8GB+")
8. click(first_result)
9. click(add_to_cart)
10. goto(cart)
11. type(coupon_code, "SAVE10")
12. click(checkout)
```

#### 评测指标

- **任务成功率**：完成任务的比例
- **步骤效率**：完成任务所需的步骤数
- **错误恢复**：遇到错误后能否恢复

#### 代表性结果

| 模型 | 任务成功率 |
|------|------------|
| GPT-4V | 14.4% |
| GPT-4 (text only) | 10.6% |
| 人类 | 78.2% |

### 3.2 Mind2Web

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2023 年 |
| 数据规模 | 2,350 个任务 |
| 网站覆盖 | 137 个真实网站 |
| 交互步骤 | 超过 10,000 步 |

#### 与 WebArena 的区别

| 维度 | WebArena | Mind2Web |
|------|----------|----------|
| 环境类型 | 可交互模拟 | 静态快照 |
| 网站来源 | 5 个克隆网站 | 137 个真实网站 |
| 评测方式 | 端到端执行 | 步骤级预测 |
| 任务多样性 | 中等 | 高 |

#### 评测指标

- **Element Accuracy**：正确识别目标元素
- **Action F1**：动作预测准确率
- **Step Success Rate**：单步成功率
- **Task Success Rate**：整体任务成功率

---

## 四、多模态 Agent Benchmark

### 4.1 VisualWebArena

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2024 年 |
| 发布机构 | CMU |
| 任务数量 | 910 个 |
| 特点 | 需要视觉理解 |

#### 设计理念

扩展 WebArena，加入需要**视觉理解**才能完成的任务。

#### 任务类型

| 类型 | 示例 |
|------|------|
| **图像识别** | "找到红色沙发的商品页面" |
| **视觉比较** | "哪个产品评分更高？"（需看星级图标）|
| **布局理解** | "点击右上角的购物车图标" |
| **图文结合** | "找与图片相似风格的商品" |

#### 评测结果

| 模型 | 成功率 |
|------|--------|
| GPT-4V | 16.4% |
| Gemini Pro Vision | 7.4% |
| Claude 3 Opus | 13.2% |

### 4.2 OSWorld

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2024 年 |
| 发布机构 | THU, CMU |
| 评测范围 | 完整操作系统 |
| 任务数量 | 369 个 |

#### 设计理念

评估 Agent 在**真实操作系统环境**中完成任务的能力，包括 Ubuntu、Windows、macOS。

#### 任务类别

| 类别 | 涉及应用 | 示例任务 |
|------|----------|----------|
| 办公 | Word, Excel, PowerPoint | 创建图表、合并单元格 |
| 开发 | VS Code, Terminal | 调试代码、安装包 |
| 浏览 | Chrome, Firefox | 网页操作、下载文件 |
| 系统 | 文件管理器、设置 | 创建文件夹、修改设置 |
| 多媒体 | GIMP, VLC | 编辑图片、播放视频 |

#### 交互方式

- **纯文本**：通过命令行指令
- **视觉+鼠标**：通过屏幕截图和模拟点击
- **混合**：结合两种方式

#### 评测结果

| 模型 | 成功率 |
|------|--------|
| GPT-4V | 12.2% |
| Gemini Pro 1.5 | 8.7% |
| Claude 3 Opus | 9.4% |
| 人类 | 72.4% |

---

## 五、工具使用 Benchmark

### 5.1 ToolBench

#### 基本信息

| 属性 | 内容 |
|------|------|
| 发布时间 | 2023 年 |
| 发布机构 | 清华大学 |
| API 数量 | 16,000+ 真实 API |
| 工具类型 | RapidAPI 平台 API |

#### 设计理念

评估 LLM 调用**真实世界 API** 的能力，包括 API 选择、参数填充、结果处理。

#### 评测维度

| 维度 | 说明 |
|------|------|
| **API 选择** | 从大量 API 中选择正确的工具 |
| **参数生成** | 根据任务正确填充 API 参数 |
| **结果处理** | 解析 API 返回并继续推理 |
| **多 API 组合** | 链式调用多个 API 完成任务 |

#### 任务示例

```
用户需求：帮我订一张从北京到上海的机票

需要调用的 API：
1. flight_search(from="Beijing", to="Shanghai", date=...)
2. user_preference(user_id=...)
3. flight_booking(flight_id=..., passenger=...)
```

---

## 六、Agent Benchmark 发展趋势

### 演进方向

```
简单任务           复杂任务
  ↓                  ↓
单步推理    →    多步规划
单一工具    →    工具组合
封闭环境    →    开放环境
文本输入    →    多模态输入
静态评测    →    动态交互
```

### 挑战与未来

| 挑战 | 现状 | 未来方向 |
|------|------|----------|
| **评测成本** | 环境搭建复杂 | 云端沙箱服务 |
| **可复现性** | 网站变化导致不可复现 | 快照存档 |
| **安全性** | Agent 可能执行危险操作 | 安全沙箱 |
| **泛化评测** | 特定任务不代表通用能力 | 更多样的场景 |

### Agent 评测框架对比

| 框架 | 关注领域 | 环境类型 |
|------|----------|----------|
| AgentBench | 综合能力 | 8 种模拟环境 |
| SWE-bench | 软件工程 | 真实代码仓库 |
| WebArena | 网页交互 | 模拟网站 |
| GAIA | 实际问题解决 | 网络 + 文件 |
| OSWorld | 操作系统 | 真实 OS |
| ToolBench | 工具调用 | 真实 API |

---

## 本节小结

1. **Agent 评测特殊性**：需要多轮交互、环境反馈、工具使用能力
2. **通用 Benchmark**：AgentBench 覆盖多场景，GAIA 关注实际问题解决
3. **代码 Agent**：SWE-bench 使用真实 GitHub Issue，是代码 Agent 的金标准
4. **Web Agent**：WebArena、Mind2Web 评估网页操作能力
5. **多模态 Agent**：VisualWebArena、OSWorld 加入视觉理解要求
6. **工具使用**：ToolBench 评估 API 调用能力

Agent 评测仍在快速发展中，现有 Benchmark 揭示了当前系统与人类能力的显著差距。

## 思考题

1. 为什么 SWE-bench 上 Agent 的表现远低于人类开发者？主要瓶颈是什么？
2. WebArena 和 Mind2Web 采用不同的评测方式（动态 vs 静态），各有什么优缺点？
3. 如果你要构建一个客服 Agent，应该用什么样的 Benchmark 来评估？现有的能满足需求吗？
